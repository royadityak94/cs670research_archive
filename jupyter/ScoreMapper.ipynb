{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as stpword\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text in image\n",
    "def jaccard_similarity(sent1, sent2):\n",
    "    union = min(len(sent1), len(sent2))\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    intersection = len(list(set(sent1).intersection(set(sent2))))\n",
    "    return (intersection / union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clean_list(original_list):\n",
    "    cleaned = list(set([wordNetLemma.lemmatize(obj_item.lower()) for item in original_list for obj_item in item.split() if obj_item not in stopwords]))\n",
    "    return [re.sub('[^a-zA-Z0-9]+', ' ', item, flags=re.UNICODE) for item in cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.image_description_scoring import matching_label_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stpword.words('english'))\n",
    "wordNetLemma = WordNetLemmatizer()\n",
    "actual1 = ['cat', 'a', 'kitty', 'near', 'tiny', 'look', 'keyboard.', 'desk,']\n",
    "actual = extract_clean_list(actual1)\n",
    "detected = extract_clean_list(['Cat', 'Mammal', 'Small to medium-sized cats', 'Whiskers', 'Felidae', 'Tabby cat', 'European shorthair', 'Carnivore', \n",
    "             'American shorthair', 'Domestic short-haired cat'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_label_score(detected, actual, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = ['people', 'train', 'next', 'driver', 'standing', 'three', 'engine', 'black']\n",
    "detected = ['vehicle', 'steam', 'transport', 'railway', 'train', 'motor', 'land', 'mode', 'locomotive', 'track', 'engine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_label_score(detected, actual, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def extract_synonyms(item, max_recursion=4):\n",
    "    synonyms = list(set([(lemma.name().lower()) for synset in wordnet.synsets(item) for lemma in synset.lemmas()]))\n",
    "    \n",
    "    for i in range(max_recursion):\n",
    "        new_list = []\n",
    "        for synonym in synonyms:\n",
    "            new_list.extend([(lemma.name().lower()) for synset in wordnet.synsets(synonym) for lemma in synset.lemmas()])\n",
    "        \n",
    "        synonyms.extend(new_list)\n",
    "        synonyms = list(set(synonyms))\n",
    "    return synonyms\n",
    "\n",
    "def get_all_synonyms(item_list, max_recursion=4):\n",
    "    synonyms = [item for item in item_list]\n",
    "    for item in item_list:\n",
    "        synonyms.extend(extract_synonyms(item, max_recursion))\n",
    "    return synonyms\n",
    "\n",
    "def matching_label_score1(detected, actual, max_depth=4):\n",
    "    total_cnt = 0\n",
    "    actual_synonyms = get_all_synonyms(actual, max_depth)\n",
    "    for item in detected: \n",
    "        if item in actual_synonyms:\n",
    "            print (item)\n",
    "            total_cnt += 1\n",
    "            continue\n",
    "        else:\n",
    "            all_synonyms = get_all_synonyms([item], max_depth)\n",
    "            for synonym in all_synonyms:\n",
    "                if synonym in actual_synonyms:\n",
    "                    print (synonym)\n",
    "                    total_cnt += 1\n",
    "                    break\n",
    "    print (total_cnt, len(actual))\n",
    "    return total_cnt/len(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as stpword\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jxx('sx', 'zs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_similarity('unison', 'osninu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "st = LancasterStemmer()\n",
    "st.stem('osninu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1, sent2 = 'unison', 'osninu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "ic_brown = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "item1 = wn.synsets(sent1, pos=wn.NOUN)#[0]\n",
    "if len(item1) > 0: \n",
    "item2 = wn.synsets(sent2, pos=wn.NOUN)#[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (sent1.lin_similarity(sent2, ic_brown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "sent1, sent2 = 'align', 'metal2222'\n",
    "\n",
    "edit_distance(sent1, sent2, substitution_cost=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\"\") if token != \"\"]\n",
    "    print (tokens)\n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = 'Alphambngjcl'\n",
    "sent1 = 'Alphzmbngjcl'\n",
    "MIN = 3\n",
    "sent1 = re.sub('[^a-zA-Z0-9]+', ' ', sent1, flags=re.UNICODE).lower().strip()\n",
    "base_weight_arr = get_base_weights(len(sent1))\n",
    "#s_arr = list(sent1)\n",
    "print (sent1)\n",
    "\n",
    "sent_grams = compute_n_grams(sent1, 3)\n",
    "actual_grams = compute_n_grams(actual.lower(), 3)\n",
    "    \n",
    "    \n",
    "# total_weight = sum([len(item)*base_weight_arr[len(item)-3] for item in gram_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_similarity(sent1, actual.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weight, computed_weight = 0, 0\n",
    "for item in sent_grams:\n",
    "    #print (item)\n",
    "    change = len(item) * base_weight_arr[len(item)-3]\n",
    "    if item in actual_grams:\n",
    "        #print ('Advantage={}'.format(item))\n",
    "        computed_weight += change\n",
    "    total_weight += change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lancasterStem = WordNetLemmatizer() #LancasterStemmer()\n",
    "\n",
    "actual1 = ['cat', 'a', 'kitty', 'near', 'tiny', 'look', 'keyboard.', 'desk,']\n",
    "actual = extract_clean_list(actual1)\n",
    "detected = extract_clean_list(['Cat', 'Mammal', 'Small to medium-sized cats', 'Whiskers', 'Felidae', 'Tabby cat', 'European shorthair', 'Carnivore', \n",
    "             'American shorthair', 'Domestic short-haired cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_label_score(detected, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(detected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms_detected = get_all_synonyms(detected, 2)\n",
    "    actual_detected = get_all_synonyms(actual, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [(lemma.name().lower()) for synset in wordnet.synsets(item) for lemma in synset.lemmas()]\n",
    "for synonym in synonyms:\n",
    "    if synonym in actual:\n",
    "        print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cnt = 0\n",
    "for item in detected1:\n",
    "    if item in actual1:\n",
    "        print (item)\n",
    "        total_cnt += 1\n",
    "        continue\n",
    "#     synonyms = [(lemma.name().lower()) for synset in wordnet.synsets(item) for lemma in synset.lemmas()]\n",
    "#     for synonym in synonyms:\n",
    "#         if synonym in actual1:\n",
    "#             print (item)\n",
    "total_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cnt, len(actual1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_label_score(detected, actual):\n",
    "    synonyms_detected = get_all_synonyms(detected, 2)\n",
    "    actual_detected = get_all_synonyms(actual, 2)\n",
    "    \n",
    "    total_cnt = [ for item in synonyms_detected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(detected1), len(actual1) l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for item in detected1:\n",
    "    #if item in actual1:\n",
    "        #total_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lancasterStem.stem('puss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.3, 0.56, 0.2], '[0.3, 0.56, 0.2]')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b, c = .3, .56, .2\n",
    "l1 = [a, b, c]\n",
    "l2 = str(l1)\n",
    "l1, l2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cs682)",
   "language": "python",
   "name": "cs682"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
