{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import xml.etree.ElementTree as ET \n",
    "from google.cloud import vision\n",
    "import requests\n",
    "from nltk.corpus import stopwords as stpword\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "from nltk.corpus import stopwords as stpword\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from utilities.image_description_scoring import matching_label_score\n",
    "from time import time\n",
    "from utilities.pythonDB import writeToDB, recordsExists\n",
    "from utilities.score_computors import jaccard_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapped(path):\n",
    "    key_map, dict_idx = {}, 0\n",
    "    for r, _, f in os.walk(path):\n",
    "        for file in f:\n",
    "            if '.JPG' in file or '.jpg' in file:\n",
    "                key_map[dict_idx] = file\n",
    "                dict_idx += 1\n",
    "    idxs = list(key_map.keys())          \n",
    "    shuffled_idxs = np.random.randint(0, len(idxs), len(idxs))\n",
    "    return key_map, shuffled_idxs\n",
    "\n",
    "def extract_clean_str(original_str):\n",
    "    original_str = original_str.replace('<br/>', '')\n",
    "    return re.sub('[^a-zA-Z0-9]+', ' ', original_str, flags=re.UNICODE).lower().strip().replace(' ', '')\n",
    "\n",
    "def fetch_from_xml_natocr(path, file):\n",
    "    tree = ET.parse(os.path.join(path, file)).getroot()  \n",
    "    return extract_clean_str(''.join([e[0].text for e in tree.iter(tag='object')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegonizeText():\n",
    "    def __init__(self, base, file): \n",
    "        with io.open(os.path.join(base, file), 'rb') as image_file:\n",
    "            content = (image_file.read())\n",
    "        self.content = content\n",
    "        self.base, self.actual_str, self.detected_str = base, '', ''\n",
    "        self.file = '_'.join(file.split('_')[-2:])\n",
    "        self.severity, self.noise_type = file.split('_')[-3], '_'.join(file.split('_')[1:-3])\n",
    "        \n",
    "        self.label = ''\n",
    "        self.start = time()\n",
    "        \n",
    "    def return_function(self, name):\n",
    "        dataset = dir_path.split('/')[1] \n",
    "        self.label = name + '-OCR-noisy'\n",
    "        if recordsExists(self.file, dataset, self.label, self.severity, self.noise_type):\n",
    "            return\n",
    "        \n",
    "        status, score =  getattr(self, 'if_' + name)()\n",
    "        compute_time = time() - self.start\n",
    "        bag = (self.file, dataset, self.label, self.severity, self.noise_type, status, score, compute_time, self.actual_str, self.detected_str)\n",
    "        print (bag)\n",
    "        #writeToDB(bag)\n",
    "    def compute_ground_truth(self):\n",
    "        ''' \n",
    "            Status = {-1: 'XML File Missing', -2: 'Error in XML', -3: 'Ground Truth Empty', -4: 'API Error', 0: 'All Correct'} \n",
    "            Return = Ground Truth, Status\n",
    "        \n",
    "        '''\n",
    "        xml_path = '/'.join(self.base.split('/')[:2]) + '/OCR'\n",
    "        xml_file = self.file[:len(self.file)-4] + '.xml' \n",
    "        \n",
    "        if not os.path.exists(os.path.join(xml_path, self.file)):\n",
    "            return '', -1\n",
    "        try:\n",
    "            ground_truth_str = fetch_from_xml_natocr(xml_path, xml_file)\n",
    "        except: \n",
    "            return '', -2\n",
    "        \n",
    "        if len(ground_truth_str) == 0:\n",
    "            return '', -3\n",
    "        else:\n",
    "            self.actual_str = ground_truth_str\n",
    "            return ground_truth_str, 0\n",
    "\n",
    "    def if_aws(self):\n",
    "        ground_truth, xml_status = self.compute_ground_truth()\n",
    "        if xml_status != 0: return xml_status, 0\n",
    "        \n",
    "        try:\n",
    "            imgobj = {'Bytes': self.content}\n",
    "            client=boto3.client('rekognition', region_name='us-east-1')\n",
    "            response=client.detect_text(Image=imgobj)\n",
    "            detected_str = extract_clean_str(''.join([txt['DetectedText'] for txt in response['TextDetections'] \\\n",
    "                                                   if txt['Type']=='WORD']))\n",
    "            self.detected_str = detected_str\n",
    "            print (\"I am here 1 \")\n",
    "            score = jaccard_similarity(detected_str, ground_truth)\n",
    "            print (\"I am here 2 \")\n",
    "            return 0, score\n",
    "        except:\n",
    "            return -4, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'datasets/neocr_dataset/noises/'\n",
    "dict_files, files_idx = get_mapped(dir_path)\n",
    "shuffled_idx = np.random.randint(0, len(files_idx), len(files_idx))[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating for File Name = add_shot_noise_4_img_941768284.jpg\n",
      "I am here 1 \n",
      "I am here 2 \n",
      "('img_941768284.jpg', 'neocr_dataset', 'aws-OCR-noisy', 0, 0.2358490566037736, 3.085570812225342, 'erlangerbergkirchweihdonnerstag27052010familienundkindertagermigtefahrundeintrittspreisebis2000uhrbetriebeerikaeuropa', 'erlangerbergkirchweihetriebedonnerstag27052010familienundkindertagintefabrundeintritispreisebis2000uhrerik')\n"
     ]
    }
   ],
   "source": [
    "for idx in shuffled_idx:\n",
    "    file_name = dict_files.get(idx)\n",
    "    print (\"Iterating for File Name = {}\".format(file_name))\n",
    "    RegonizeText(dir_path, file_name).return_function('aws')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'datasets/neocr_dataset/noises/'\n",
    "dict_files, files_idx = get_mapped(dir_path)\n",
    "shuffled_idx = np.random.randint(0, len(files_idx), len(files_idx))[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'add_impulse_noise_4_img_297611753.jpg'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = dict_files.get(0)\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('000000491277.jpg', -1, 'image_eraser')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'random_image_eraser_000000491277.jpg'\n",
    "file_name, severity, noise_type = '', '', ''\n",
    "if 'img' in file:\n",
    "    file_name = '_'.join(file.split('_')[-2:])\n",
    "    if 'eraser' in file:\n",
    "        severity, noise_type = -1, '_'.join(file.split('_')[1:-2])\n",
    "    else:\n",
    "        severity, noise_type = file.split('_')[-3], '_'.join(file.split('_')[1:-3])\n",
    "else:\n",
    "    file_name = '_'.join(file.split('_')[-1:])\n",
    "    if 'eraser' in file:\n",
    "        severity, noise_type = -1, '_'.join(file.split('_')[1:-1])\n",
    "    else:\n",
    "        severity, noise_type = file.split('_')[-2], '_'.join(file.split('_')[1:-2])\n",
    "    \n",
    "file_name, severity, noise_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'perform_swirl_transformation_4_000000276735.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'000000427438.jpg'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = '_'.join(file.split('_')[-1:])\n",
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4', 'gaussian_noise')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "severity, noise_type = file.split('_')[-2], '_'.join(file.split('_')[1:-2])\n",
    "severity, noise_type"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cs682)",
   "language": "python",
   "name": "cs682"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
